在CV中，一开始的时候，每个patch规定都是196*768，之所以是这个数，因为默认输入的是224*224*3的输入图像，并按每个patch都是16*16划分，因为有3通道，因为实际上每个patch是16*16*3，然后展开成序列，变成768
而224*224/（16*16）=14*14个patch，展开后是196,所以就是196*768，即有196个patch，每个patch的维度是768。在transformer中就相当于有196个单词，每个单词用768个维度来表示。

输入图像后，cat上一个新建的专用于分类的token（1*768），变成了197*768（输入嵌入），然后在通过余弦相似度创建197个位置信息（这样通过公式创建的位置信息，只要输入是197*768，那么位置信息都是一致的），维度都是768。
然后将该位置信息（197*768）一一叠加到输入嵌入上。每个patch（1*768）仅叠加位置信息的其中一个1*768，并且197个位置信息都是不一样的。这个位置信息即可用余弦求得，也可直接学习获得。

叠加位置信息后，经过dropout和LN归一化，开始进入自注意力：
新的输入经过线性层生成QKV，Q,K,V都是由新的输入点积WQ,WK,WV得到，之所以是自-注意力，这个自就是因为QKV都是来自新的输入，而没有其他信息。还有就是所有Q都会和所有K进行匹配。相当于每个Q/patch都匹配或者说使用了全部的patch的k信息。
正常情况下，我们所说的Q和K点积，实际上是Q点积K的转置。一般默认生成的Q,K,V在维度（形状）上是一致的，所以点积的时候Q要点积K的装置，不然如果Q,K都是1行2列，K不转置根本无法计算。
而新的输入和WQ,WK,WV也是同理，之所以看不出转置的操作，因为WQ,WK,WV是创建而来，所以可以认为其实已经经过了转置。
每一个输入/patch都是经过同一个线性层。也就是不同的patch都是使用同一套线性层参数。但是该线性层包含WQ,WK,WV，这3个是不同的，所以一个patch经过线性层得到的Q,K,V是不同的。
按照深度学习的无法解释理论来说。如果WQ,WK,WV真的学习成功了，也就是在结果上loss很低，准确率很高，那么此时我们应该认为WQ学习到了如何从新的输入处获得查询的能力，
这个查询是patch特有的一种想要表达出自身的信息的东西,WK学习到了如何从新的输入处获得一种能代表被关注匹配的能力,WV学习到了如何从新的输入处获得该patch真正重要的信息的能力。

仔细分析上面Q和K的点积，在没有将每个patch合成为一个矩阵之前，q需要和每个patch的k的转置进行点积运算，然后再经过除以维度（缩放）和softmax得到一个注意力分数，有197个patch，也就是对于每一个patch都能得到197个注意力分数，
每个分数都是该q对相应计算的patch的v的关注度。
如果在将每个patch合成为一个矩阵之后，仅看Q，如果Q的每一行代表一个patch（也就是一行有768个维度，有197行），那么最后得到的注意力分数矩阵（197*768点积768*197=197*197）的每一行也是一个patch，
一行内容就是该patch对全部patch的v的注意力分数。现在的197*197的第1列是每个patch对第1个patch的关注，反过来说就是第1列是第1个patch获得每一个patch的关注度。

在上面这个的基础上，再讨论(QK)点积V。
对于上面得到的注意力分数矩阵（197*197），第1行代表了第1个patch，第1列代表了第1个patch，1行1列代表了第1个patch对第1个patch的v的关注度。

Q,K,V都是来自新的输入，一般来说Q,K,V的形状是一样的，上面Q和K是197*768，Q与K转置点积后是197*197，此时V如果是197*768，也就是再与V点积结果的形状会回到197*768。

对于V来说，每一行代表一个patch，该行的所有列代表了该patch的V信息。

此时注意力矩阵（197*197）与V点积，注意力矩阵实质上就是V的权重矩阵，在将每一个v合成矩阵后，计算不再是对应元素相乘，因为(197*197)逐元素相乘(197*768)，两个形状不一样，根本无法计算。

最后的结果维度是【(197*197)点积(197*768)=(197*768)】，在注意力矩阵与V点积的过程中，注意力矩阵的每一行/每一个patch（1*197）逐一与含全部patch信息的V（197*768）进行点积得到（1*768），这就是该patch对全局patch的信息进行综合考虑后，
得到了该patch在768个维度上的实际分数。现在得到的这些分数，每个不仅仅代表了特征，其大小还代表了在整体得到的关注大小。也就是增强了需要关注的特征，抑制了那些不重要的特征，比如说分类时的背景所得出的特征。考虑的全是重点。
同理，还有196个patch也都要与全部patch的V（197*768）点积，得到196个（1*768）的分数。最后合成最终输出（197*768）。

多头自注意力：
在新的输入得到Q,K,V后，如果要进行多头（分组）处理，比如说是分4个头（组），那么就对Q进行维度上的拆分，将Q的总维度拆分成4组，比如说Q是（197*768），即有197个patch，每个patch有768个维度。将这768拆分成（192+192+192+192），
即得到了Q1,Q2,Q3,Q4，维度都是（197*192）。这个拆分的过程可以是直接均分，也可以是通过线性层映射。

这样，在每个头都得到最后的结果后，进行concat拼接。但是这里还要再进行一个融合操作，不是前面简单的concat合并，因为每个头在计算的时候虽然考虑了全部的patch，但是每个头所拥有的patch都是不完整的。那么concat后，如果每个头有
颜色的话，可以看出是泾渭分明的。所以我们还要进行一次全部信息的整合。这一步通过点积一个可学习矩阵来完成。
