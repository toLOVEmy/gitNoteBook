## 我对transformer的理解：
## 参考文献：[Transformer从零详细解读(可能是你见过最通俗易懂的讲解)](https://www.bilibili.com/video/BV1Di4y1c7Zm?p=3&vd_source=c56a3545dad1dc254570b76b0698c3c1)
### 总体结构：输入层-编码器-解码器-输出
### 1、输入层（词嵌入+位置编码）
![image](https://github.com/user-attachments/assets/c32682fb-545d-40a7-b22a-29f05934293a)
#### 1.1、独热编码
- 输入“我爱你”，经过独热编码，在字典的位置，举例：一般‘我’是‘000..010..00’，在字典‘我’的位置标1，其余标0。
- ![image](https://github.com/user-attachments/assets/440951c9-aa0d-44b5-a73d-8d120d0796dc)
#### 1.2、词嵌入
- 输入“我爱你”，其中每个最小语义token，在中文中一般就是每个字，经过独热编码后，再分别经过同一个固定不变的静态嵌入矩阵，分别得到固定长度（dmodel=512）的1×512的单词向量。`因为输入是3个token，那么现在得到了3×512的向量矩阵`
- ![image](https://github.com/user-attachments/assets/c6584776-74ba-4418-ab55-e82383fc9157)
#### 1.3、位置编码
- 如果仅通过上面的静态嵌入矩阵，那么‘爱’这个字放在哪里得到的1×512单词向量都是一样的，也就是“爱你我”和“我爱你”中的‘爱’输入进静态嵌入矩阵得到是同一语义（1×512向量），这是我们不希望看到的，因为他们位置不一样。
- 现在构建一个1×512的位置向量，这个512只需和上面的固定长度向量长度一样即可，目的是为了后面词嵌入输出和位置编码输出的相加。而且构造的向量和内容没关系，只和位置有关系，根据transformer的公式，位置向量内容只和`固定长度dmodel，位置pos（包含了奇偶位置），sin，cos`有关，也就是只要确定了这个公式和固定长度，那么无论谁来构建这个位置向量，都是一样的。
#### 1.4、词嵌入输出+位置编码输出
- 输入“我爱你”，每个词（token）分别经过同一个固定不变的静态嵌入矩阵，得到3×512的向量矩阵
- 输入“我爱你”，每个词（token）分别经过同一个1×512的位置向量，根据位置，分别是第1，第2，第3个位置，分别得到[1,0,0,0...],[0,1,0,0...],[0,0,1,0...]即3个1×512单词向量,合起来也是3×512的向量矩阵
- 将上面两个3×512的向量矩阵相加得到一个新的3×512的向量矩阵，作为编码器的输入
### 2、编码器
![image](https://github.com/user-attachments/assets/a29a8d4e-bfa6-4642-b66b-e84e582dc669)
#### 2.1、注意力机制（多头注意力机制）
![image](https://github.com/user-attachments/assets/abec7d71-d81f-4181-a421-271420e9522b)
#### 2.1.1、Q,K,V的获取
![image](https://github.com/user-attachments/assets/52c404bb-2d6b-4d97-904d-6d58be0b5ba3)
- 经过输入层输入进来的3×512的向量矩阵X，是3个1×512的单词向量X1,X2,X3，即X（3×512）由X1（1×512）,X2（1×512）,X3（1×512）拼凑而来。
- 现在创建3个权重矩阵，分别是Wk（512×128）,Wq（512×128）,Wv（512×128），分别用于学习生成怎样的Q（3×128）,K（3×128）,V（3×128）。
- X1（1×512）通过分别与Wk（512×128）,Wq（512×128）,Wv（512×128）点乘分别得到X1的Q1（1×128）,K1（1×128）,V1（1×128）。
- 同理X2和X3也能得到Q2（1×128）,K2（1×128）,V2（1×128）和Q3（1×128）,K3（1×128）,V3（1×128）。Q1（1×128）,Q2（1×128）,Q3（1×128）拼凑起来就是Q（3×128）。K,V同理。
#### 2.1.2、计算QK相似度，得到attention值
![image](https://github.com/user-attachments/assets/f7bf4cfc-4e34-45d7-a515-a41200eedd4f)
- 得到Q1,K1,V1后，先不急着合并成Q,K,V
- `先用q1,k1,v1代替上面的Q1,K1,V1`
- q1点乘`（一根线/向量在另一根线/向量上的投影长度，表示相关度，90°自然投影为0，不相关）`k1的转置得到score，`因为score可能很小，为了梯度不消失，并且保持方差为1`，除以根号下dk，`再为了使权重归一化，即所有权重加起来等于1，相当于按不同比例划分注意力`，再经过soft-max层输出attention值。
- 每个单词向量（token）所对应的V，我理解是单词语义中真正的语义部分。也就是原来可能还包含一下其他东西。比如注意力的比例因子。
- ![image](https://github.com/user-attachments/assets/0296fd5c-c77a-4faa-9389-018b7e713cc8)
- 现在得到了每个单词所需要被关注的注意力权重，那么让attention值*v1得到z1。即第1个单词的语义，以及该语义需要被关注的程度。
- 第2,3单词同理得到z2,z3。然后合成这套注意力机制的输出Z1。
- 在实际代码中，一般是多个单词向量并行（形成矩阵），所以和上面所说有一点不同，相当于结果等同于`得到Q1,K1,V1后，直接合并成Q,K,V`，可以看到是2个单词，每个单词向量是1×4,Wk（4×3）,Wq（4×3）,Wv（4×3），得到的Q1（1×3）和Q2（1×3）拼凑的Q是2×3。
- 一套注意力机制相当于从`一种角度`去认为这句话的每个单词应该得到多少关注
- ![image](https://github.com/user-attachments/assets/a8adca39-fe68-4c50-bc59-0fee7bb225ec)
#### 2.2、多头注意力机制
![image](https://github.com/user-attachments/assets/76c4c711-43cf-4606-9a04-98107e751713)
- 为了形成类似CNN那种可以从不同角度观察的效果（多个卷积层），这里也并行使用多套注意力机制（多头注意力机制）
- 上面红框就是一套注意力机制，蓝框是另一套
- 上面是双重并行，也就是多套注意力机制并行。还有就每套单词也并行。
- 第1套生成Z1....第7套生成Z7。
- ![image](https://github.com/user-attachments/assets/e0612b5a-c0a7-4b9a-b3fe-a0beeb30e50b)
#### 2.3、残差
- 输入层输出的X（3×512）与注意力机制输出的Z（3×512）相加`再输入到LayerNorm层`。`可能和上图有些不符合`
#### 2.4、LayerNorm层
![image](https://github.com/user-attachments/assets/69f336f0-baa3-4e82-b450-776b25c6ccb4)
- 先说为什么不使用BN层，上图是BN层的图
- BN层是取一批图像中每一张图像在同一个位置的值做均值和方差，即用一批(batchsize)图像的均值和方差来模拟整体图像的均值和方差，当batch-size过小自然就不准确了
- 如果放在NLP中，每句话的长度不一样，如果有些话太短，那么剩下的长度都是用0填充，起不到效果。比如batchsize=10，只有一句话长度是10，其余都小于10，那么在第10个维度，相当于这一批在第10个维度起作用的只有1条数据，用1条数据的第10个维度去模拟整体数据的第10个维度，偏差大的离谱。
- 而LN层则是对每一条数据的所有维度取均值和方差，只需要考虑每一条数据就行。
#### 2.2、前馈神经网络
- 两个全连接层+一个残差
### 3、解码器
#### 3.1、Mask层
- 因为transformer拥有看见全局的能力，但是在实际预测中，按道理我们是不知道全局的，因此必须在训练的过程中通过Mask失去这种能力
#### 3.2、交互层
- 每一个解码器会接受来自两个输入，一个是来自上一个解码器的输出（接受Q），一个是来自编码器的最终输出（一个残差，接受K,V）
### 4、输出
